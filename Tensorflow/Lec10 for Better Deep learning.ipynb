{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* activation function: 각 unit의 sigmoid 같은 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation의 문제\n",
    "- layer가 많아질수록 accuracy가 안 좋다. \n",
    "- Vanishing gradient: 어떤 값의 sigmoid는 0~1 사이일 것이고, 앞쪽의 layer의 경우 곱해지는 값이 굉장히 작아지게 된다. 앞으로 갈수록 점차 경사도가 안 좋아진다.\n",
    "- 원인\n",
    "    1. We used wrong type of non-linearity (by Geoffrey Hinton)\n",
    "    2. We initialized the weights in a stupid way\n",
    "    \n",
    "   \n",
    "\n",
    "### Solution1 : ReLU(Rectified Linear Unit)\n",
    "<img src=\"relu.png\">\n",
    "\n",
    "#### NN에서는 이제 sigmoid 대신 relu를 사용\n",
    "마지막 단에서는 0~1의 값이 필요하니까 sigmoid를 사용!\n",
    "\n",
    "##### 다른 종류: Leaky ReLu   /   Maxout   /   ELU   /   tanh(sigmoid 보완)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution2\n",
    "1. Not all 0's\n",
    "2. 논문: 'A Fast Learning Algorithm for Deep Belief Nets' (이제 많이 사용 X)\n",
    "    - Resticted Boatman Machine (RBM)\n",
    "    \n",
    "### How can we use RBM to initiailze weights?\n",
    "1. Apply the RMB idea on adjacent two layers as a pre-training step\n",
    "2. Continue the first process to all layers\n",
    "3. This will set weights\n",
    "\n",
    "### Good News\n",
    "- Simple methods are OK\n",
    "\n",
    "#### Xavier/He initialization\n",
    "- Makes sure the wieghts are 'just right', not too small, not too big\n",
    "- Using number of input (fan_in) and output (fan_out)\n",
    "    1. W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n",
    "    2. W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN dropout and model ensemble\n",
    "\n",
    "이것을 하는 이유? Overfitting\n",
    "\n",
    "* Am I overfitting ?\n",
    "    1. Very high accuracy on the training dataset\n",
    "    2. But poor accuracy on the test data set\n",
    "    \n",
    "- layer의 층을 늘릴수록, train error는 계속 작아지지만 test error는 줄어들다가 어느 순간 다시 커지게 된다. \n",
    "\n",
    "* Solution\n",
    "    1. More Training Data\n",
    "    2. Reduce the # of features\n",
    "    3. Regularization  (+Dropout)\n",
    "  \n",
    "  \n",
    "### Regularization: Dropout - randomly set some neuros to zero in the forward pass\n",
    "- 학습: dropout <-> 실전: dropout_rate = 1\n",
    "\n",
    "\n",
    "### Ensemble?\n",
    "<img src=\"ensem.png\">\n",
    "출처: http://www.slideshare.net/sasasiapacific/ipb-improving-the-models-predictive-power-with-ensemble-approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various ways to stack NN module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feedforward neural network\n",
    "\n",
    "### 2. Fast forward\n",
    "- signal을 두 단 앞으로 보내기\n",
    "\n",
    "### 3. Split & merge\n",
    "\n",
    "\n",
    "### 4. Recurrent network (RNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN for MNIST\n",
    "- softmax보다 단을 더 많이! + ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-d3560a247456>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True) #label을 one_hot으로 바꾸기\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, nb_classes]))\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "hypothesis = tf.matmul(L2, W3) +b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost/loss & Optimizer\n",
    "learning_rate = 0.01\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                                             labels = Y))\n",
    "\n",
    "#AdamOptimizer : uses moving averages of the parameters (momentum)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001  cost =  48.531320539\n",
      "Epoch:  0002  cost =  9.034787826\n",
      "Epoch:  0003  cost =  4.922225090\n",
      "Epoch:  0004  cost =  3.301491155\n",
      "Epoch:  0005  cost =  2.714303530\n",
      "Epoch:  0006  cost =  2.484510562\n",
      "Epoch:  0007  cost =  2.334515521\n",
      "Epoch:  0008  cost =  1.746345760\n",
      "Epoch:  0009  cost =  1.806353526\n",
      "Epoch:  0010  cost =  1.512919767\n",
      "Epoch:  0011  cost =  1.351888039\n",
      "Epoch:  0012  cost =  1.231365173\n",
      "Epoch:  0013  cost =  0.959691505\n",
      "Epoch:  0014  cost =  1.087991192\n",
      "Epoch:  0015  cost =  0.963870677\n",
      "Accuracy: 0.9589\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Training cycle:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) #iteration 횟수\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #100개씩 data 읽어오기\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict = {X:batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch: ', '%04d' % (epoch +1), ' cost = ','{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    #test the model using test sets\n",
    "    print('Accuracy:', accuracy.eval(session = sess, feed_dict = {X: mnist.test.images,\n",
    "                                                                 Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xaiver initialization\n",
    "- 처음부터 cost가 낮다: 초기값 설정이 잘 되었음을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #tf는 기존의 변수를 지울 수 없음. \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = [784,256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape = [256, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [256, nb_classes],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost/loss & Optimizer\n",
    "learning_rate = 0.01\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                                             labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001  cost =  0.261068299\n",
      "Epoch:  0002  cost =  0.135333015\n",
      "Epoch:  0003  cost =  0.118715815\n",
      "Epoch:  0004  cost =  0.105170587\n",
      "Epoch:  0005  cost =  0.093711926\n",
      "Epoch:  0006  cost =  0.095917307\n",
      "Epoch:  0007  cost =  0.083916248\n",
      "Epoch:  0008  cost =  0.081445102\n",
      "Epoch:  0009  cost =  0.079361560\n",
      "Epoch:  0010  cost =  0.069836883\n",
      "Epoch:  0011  cost =  0.074417542\n",
      "Epoch:  0012  cost =  0.077027685\n",
      "Epoch:  0013  cost =  0.066237198\n",
      "Epoch:  0014  cost =  0.049401297\n",
      "Epoch:  0015  cost =  0.073024502\n",
      "Accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Training cycle:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) #iteration 횟수\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #100개씩 data 읽어오기\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict = {X:batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch: ', '%04d' % (epoch +1), ' cost = ','{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    #test the model using test sets\n",
    "    print('Accuracy:', accuracy.eval(session = sess, feed_dict = {X: mnist.test.images,\n",
    "                                                                 Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NN for MNIST\n",
    "- 3단에서 5단으로! node의 수도 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = [784,512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape = [512, 512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [512, 512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "\n",
    "W4 = tf.get_variable('W4', shape = [512, 512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "\n",
    "W5 = tf.get_variable('W5', shape = [512, nb_classes],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([nb_classes]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost/loss & Optimizer\n",
    "learning_rate = 0.01\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                                             labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001  cost =  0.488696199\n",
      "Epoch:  0002  cost =  0.190954977\n",
      "Epoch:  0003  cost =  0.158582509\n",
      "Epoch:  0004  cost =  0.142573393\n",
      "Epoch:  0005  cost =  0.134667423\n",
      "Epoch:  0006  cost =  0.127765756\n",
      "Epoch:  0007  cost =  0.111130574\n",
      "Epoch:  0008  cost =  0.114275015\n",
      "Epoch:  0009  cost =  0.111815214\n",
      "Epoch:  0010  cost =  0.106692245\n",
      "Epoch:  0011  cost =  0.116992086\n",
      "Epoch:  0012  cost =  0.097166270\n",
      "Epoch:  0013  cost =  0.100400571\n",
      "Epoch:  0014  cost =  0.127970845\n",
      "Epoch:  0015  cost =  0.108218026\n",
      "Accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Training cycle:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) #iteration 횟수\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #100개씩 data 읽어오기\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict = {X:batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch: ', '%04d' % (epoch +1), ' cost = ','{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    #test the model using test sets\n",
    "    print('Accuracy:', accuracy.eval(session = sess, feed_dict = {X: mnist.test.images,\n",
    "                                                                 Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존보다 Deep하게 쌓았음에도 불구하고, 이전보다 Accuracy가 낮음.\n",
    " - overfitting이 그 원인\n",
    " \n",
    "이를 예방하기 위해 Dropout을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout for MNIST\n",
    "- 통상적으로 keep_prob = 0.5~0.7\n",
    "- Test할 때는 반드시 keep_prob = 1\n",
    "- 그래서 keep_prob를 placeholder로 둔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True) #label을 one_hot으로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "#dropout rate 0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = [784,512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape = [512, 512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [512, 512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "W4 = tf.get_variable('W4', shape = [512, 512],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "W5 = tf.get_variable('W5', shape = [512, 10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "hypotheis = tf.nn.dropout(L4, keep_prob = keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost/loss & Optimizer\n",
    "learning_rate = 0.001  #이전보다 learning rate 더 작게\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis,\n",
    "                                                             labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001  cost =  0.454613173\n",
      "Epoch:  0002  cost =  0.171633255\n",
      "Epoch:  0003  cost =  0.132865671\n",
      "Epoch:  0004  cost =  0.104946284\n",
      "Epoch:  0005  cost =  0.093991493\n",
      "Epoch:  0006  cost =  0.082176484\n",
      "Epoch:  0007  cost =  0.075727579\n",
      "Epoch:  0008  cost =  0.065757217\n",
      "Epoch:  0009  cost =  0.063511627\n",
      "Epoch:  0010  cost =  0.059284707\n",
      "Epoch:  0011  cost =  0.056172906\n",
      "Epoch:  0012  cost =  0.048765170\n",
      "Epoch:  0013  cost =  0.051348418\n",
      "Epoch:  0014  cost =  0.045860301\n",
      "Epoch:  0015  cost =  0.046133322\n",
      "Accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Training cycle:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) #iteration 횟수\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #100개씩 data 읽어오기\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict = {X:batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch: ', '%04d' % (epoch +1), ' cost = ','{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    #test the model using test sets\n",
    "    print('Accuracy:', accuracy.eval(session = sess, feed_dict = {X: mnist.test.images,\n",
    "                                                                 Y: mnist.test.labels,\n",
    "                                                                 keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "<img src=\"opt.png\">\n",
    "\n",
    "- 무엇이 좋은지 simulation할 수 있는 사이트\n",
    "    - http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "    \n",
    "- 통상적으로 Adam Optimizer가 좋음\n",
    "\n",
    "\n",
    "<img src=\"sum.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
